import sys
import scipy
import numpy
import matplotlib
import pandas
import sklearn

##Loading Libraries
import pandas 
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

##Import Data from UCI repo using pandas
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
##Columns from the dataset
names = ['sepal-length','sepal-width','petal-length','petal-width','class']
##Loading the dataset using pandas
dataset = pandas.read_csv(url,names=names);

##printing rows and columns of the dataset
print(dataset.shape)

##prinitng first 30 instances of the dataset
print(dataset.head(30))

##printing mean, mode and other percentiles
print(dataset.describe())

##prinitng size of each class
print(dataset.groupby('class').size())

##plotting uni-varied graphs for understanding each attribute
dataset.plot(kind="box", subplots=True, layout=(2,2), sharex=False, sharey=False)
plt.show()

##plotting a histogram
dataset.hist()
plt.show()

##Multi-varied plot to see the interaction between the different variables
scatter_matrix(dataset)
plt.show()  ##This plot shows High Co-relation and predictability

##Creating a validation dataset
array = dataset.values
X = array[:,0:4] ##This brings the columns from starting till the end
Y = array[:,4] ##This is the class for the output, so we're just grabbing the last column in the dataset
validation_size = 0.20 ##This means that 20% of the data would be held back as our validation dataset
seed = 6 ##From where the numbers would start in generating random numbers
X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X,Y,test_size=validation_size,random_state=seed)

##creating test harness
##for this we'll be using ten-fold cross validation to check the acuuracy
seed = 6
scoring = 'accuracy' ##ratio of correctly predicted instances divided by the total number of instances in the dataset

##Building a model
##SPOT CHECK algorithms

models=[]
models.append(('LR',LogisticRegression()))
models.append(('LD',LinearDiscriminantAnalysis()))
models.append(('KNN',KNeighborsClassifier()))
models.append(('CART',DecisionTreeClassifier()))
models.append(('NB',GaussianNB())) ##naive-bayes
models.append(('SVM',SVC()))  ##“Support Vector Machine” (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges.

##evaluate each model in turn

results = []
names = []

for name, model in models:
  kfold = model_selection.KFold(n_splits=10, random_state=seed)
  cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring )
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(),cv_results.std())
  print(msg)



